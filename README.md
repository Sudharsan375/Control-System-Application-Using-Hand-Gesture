PROJECT : I developed a Hand Gesture Recognition System that enables touchless control of computer operations using real-time hand tracking. The project uses OpenCV for video capture and MediaPipe’s Hand Landmark Detection algorithm to identify 21 critical points on the hand. To recognize gestures, I designed a distance-based feature extraction method, where I calculate signed distances and relative ratios between key landmarks. Gestures like palm open, fist, V-sign, and pinch (both major and minor) are classified using simple geometric and ratio-based techniques. For system control, I mapped each gesture to actions such as cursor movement, left-click, right-click, double-click, drag-and-drop, vertical and horizontal scrolling, and adjustment of system volume and screen brightness. To ensure a smooth user experience, I implemented noise filtering by verifying gesture consistency over multiple frames and applied motion dampening to stabilize rapid hand movements. Libraries like PyAutoGUI were used for automating mouse and keyboard events, PyCAW for controlling system audio, and Screen Brightness Control for monitor adjustments. Overall, this project demonstrates my ability to combine computer vision algorithms, real-time data processing, and system-level integration to create a seamless and intuitive human-computer interaction system. For gesture recognition, I used MediaPipe’s hand tracking model to detect 21 hand landmarks with high accuracy. I designed a lightweight custom algorithm that analyzes the relative distances and signed angles between landmarks to determine which fingers are open or closed. Based on the finger states, I encoded gestures like a fist, palm, V-sign, and pinch using binary and geometric rules. To further enhance reliability, I implemented a frame consistency technique where a gesture must remain stable for a few consecutive frames before being confirmed, reducing false positives due to hand movement or camera noise. Additionally, to distinguish similar gestures, I compared depth (z-axis) differences and landmark ratios, ensuring fine-grained classification between open fingers and closed ones. This combination of spatial analysis and temporal smoothing allowed the system to recognize gestures accurately in real-world, real-time scenarios. For more details, you can refer to our findings published in the journal  http://www.ijrar.org/papers/IJRA 
